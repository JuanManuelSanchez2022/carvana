{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FmnAFnm4imHA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "#\n",
        "import PIL\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGa0fHfEjnnq"
      },
      "source": [
        "##GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyjgib12jlEj",
        "outputId": "b49bc768-d71a-4a67-b33c-2ec949a721c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHbvjaSej5MX"
      },
      "source": [
        "#Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: montar drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIfPRrewj9Af",
        "outputId": "4f951811-3a13-449a-ca7f-5ba9e20239cc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Mtzbt1F3j286"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/Deep Learning/Carvana'\n",
        "TRAIN_PATH = '/content/drive/MyDrive/Deep Learning/Carvana/train'\n",
        "TRAIN_MASKS_PATH = '/content/drive/MyDrive/Deep Learning/Carvana/train_masks'\n",
        "TEST_PATH = '/content/drive/MyDrive/Deep Learning/Carvana/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SADTsWYrlRxb",
        "outputId": "c9b01dbe-bca3-4cac-de5f-47c6b0e7465e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# prompt: verificar si se lee correctamente desde drive el archivo (PATH, TRAIN, TRAIN_MASKS Y TEST)\n",
        "import os\n",
        "print(os.path.exists(PATH))\n",
        "print(os.path.exists(TRAIN_PATH))\n",
        "print(os.path.exists(TRAIN_MASKS_PATH))\n",
        "print(os.path.exists(TEST_PATH))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j18phXbkIZq"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "w8hxEsQFkK-2"
      },
      "outputs": [],
      "source": [
        "# creating our own Dataset\n",
        "class Car_Dataset(Dataset):\n",
        "    def __init__(self, data, masks=None, img_transforms=None, mask_transforms=None):\n",
        "        '''\n",
        "        data - train data path\n",
        "        masks - train masks path\n",
        "        '''\n",
        "        self.train_data = data\n",
        "        self.train_masks = masks\n",
        "\n",
        "        self.img_transforms = img_transforms\n",
        "        self.mask_transforms = mask_transforms\n",
        "\n",
        "        self.images = sorted(os.listdir(self.train_data))\n",
        "        self.masks = sorted(os.listdir(self.train_masks))\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train_masks is not None:\n",
        "            assert len(self.images)==len(self.masks), 'not the same number of images and masks'\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = os.path.join(self.train_data, self.images[idx])\n",
        "        img = Image.open(image_name)\n",
        "        trans = T.ToTensor()\n",
        "        if self.img_transforms is not None:\n",
        "            img = self.img_transforms(img)\n",
        "        else:\n",
        "            img =trans(img)\n",
        "\n",
        "        if self.train_masks is not None:\n",
        "            mask_name = os.path.join(self.train_masks, self.masks[idx])\n",
        "            mask = Image.open(mask_name)\n",
        "            if self.mask_transforms is not None:\n",
        "                mask = self.mask_transforms(mask)\n",
        "            else:\n",
        "                mask = trans(mask)\n",
        "\n",
        "            mask_max = mask.max().item()\n",
        "            mask /= mask_max\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU9qF4pSnC5C"
      },
      "source": [
        "#Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Fvy_YdR1nFQN"
      },
      "outputs": [],
      "source": [
        "transform_data = T.Compose([\n",
        "                T.Resize([224, 224]),\n",
        "                T.ToTensor()] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUFqwkwMnWHD"
      },
      "source": [
        "#Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Gi8hc15dnVYq"
      },
      "outputs": [],
      "source": [
        "full_dataset = Car_Dataset(TRAIN_PATH,\n",
        "                           TRAIN_MASKS_PATH,\n",
        "                           img_transforms=transform_data,\n",
        "                           mask_transforms=transform_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(full_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1sYcg9w70eo",
        "outputId": "55ba9044-b561-4d19-e57a-9a71fbfb2244"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uv_2Mi-XntWp"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "TRAIN_SIZE = int(len(full_dataset)*0.8)\n",
        "VAL_SIZE = len(full_dataset) - TRAIN_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(full_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV_41VySzf7k",
        "outputId": "bad3d402-cf7b-44a3-a705-269fd686ccc8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IS98VMZnn9sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90821060-57d2-4568-ba55-83f213ef9c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n"
          ]
        }
      ],
      "source": [
        "print(TRAIN_SIZE, VAL_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "T1n-bCwRoD1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c698831-5bb4-4e52-c358-34df146b9de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:414: UserWarning: Length of split at index 0 is 0. This might result in an empty dataset.\n",
            "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
          ]
        }
      ],
      "source": [
        "train_dataset, val_dataset=random_split(full_dataset, [TRAIN_SIZE, VAL_SIZE])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QMYiw2SBoO7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05575c7-3b22-4bf8-b4b2-3ec90f9c5c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset), len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "r0U8q9wGoYbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "f09ec495-1370-4208-b1da-097e8fd59163"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-719c826cb180>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb-R0OaXo1Tp"
      },
      "outputs": [],
      "source": [
        "imgs, masks = next(iter(train_loader))\n",
        "print(imgs.shape, masks.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-DIFyLZo_DA"
      },
      "outputs": [],
      "source": [
        "for i, (x, y) in enumerate(train_loader):\n",
        "    print(i, x.shape, y.shape)\n",
        "    if i==9: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6vTbXpOpNwX"
      },
      "source": [
        "#Let us see the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1khk-Nf-pQrb"
      },
      "outputs": [],
      "source": [
        "imgs, masks = next(iter(train_loader))\n",
        "def plot_mini_batch(imgs, masks):\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for i in range(BATCH_SIZE):\n",
        "        plt.subplot(4, 8, i+1)\n",
        "        img = imgs[i, ...].permute(1, 2, 0).numpy()\n",
        "        mask = masks[i, ...].permute(1, 2, 0).numpy()\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(mask, alpha=0.5)\n",
        "\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_mini_batch(imgs, masks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EItLbXdxqhXi"
      },
      "source": [
        "#Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz_-zEosqijB"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, loader):\n",
        "    correct = 0\n",
        "    intersection = 0\n",
        "    denom = 0\n",
        "    union = 0\n",
        "    total = 0\n",
        "    cost = 0\n",
        "    model = model.to(device=device)\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=torch.float32)\n",
        "            y = y.to(device=device, dtype=torch.long).squeeze(1)\n",
        "            scores = model(x)\n",
        "            cost += (F.cross_entropy(scores, y)).item()\n",
        "          #standard accuracy not optimal\n",
        "            preds = torch.argmax(scores, dim=1)\n",
        "            correct += (preds == y).sum()\n",
        "            total += torch.numel(preds)\n",
        "          #dice coefficient\n",
        "            intersection += (preds*y).sum()\n",
        "            denom += (preds+y).sum()\n",
        "            dice += 2*intersection / (denom + 1e-8)\n",
        "          #intersection over union\n",
        "            union += (preds + y - preds*y).sum()\n",
        "            iou = (intersection) / (union + 1e-8)\n",
        "\n",
        "        return cost / len(loader), float(correct) / total, dice, iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKkNDLTwsyLq"
      },
      "outputs": [],
      "source": [
        "len (train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyG3qdrzs1Fg"
      },
      "source": [
        "#Search dor learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTEYetC9s4cE"
      },
      "outputs": [],
      "source": [
        "def find_lr(model, optimiser, start_val=1e-6, end_val=1, beta=0.99, loader=train_loader):\n",
        "    n = len(loader) - 1\n",
        "    factor = (end_val / start_val)**(1/n)\n",
        "    lr = start_val\n",
        "    optimiser.param_groups[0]['lr'] = lr #this allows you to update the Learning rate\n",
        "    avg_loss, loss, acc = 0., 0., 0.\n",
        "    lowest_loss = 0.\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "    accuracies = []\n",
        "    model = model.to(device=device)\n",
        "    for i, (x, y) in enumerate(loader, start=1):\n",
        "        x = x.to(device=device, dtype=torch.float32)\n",
        "        y = y.to(device=device, dtype=torch.long).squeeze(1)\n",
        "        optimiser.zero_grad()\n",
        "        scores = model(x)\n",
        "        cost = F.cross_entropy(input=scores, target=y)\n",
        "        loss = beta*loss + (1-beta)*cost.item()\n",
        "       #bias correction\n",
        "        avg_loss = loss / (1-beta**i)\n",
        "\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        acc_ = (preds == y).sum() / torch.numel(scores)\n",
        "      #acc = beta*acc + (1-beta) * acc_.item()\n",
        "      #avg_acc = acc / (1-beta**i)\n",
        "      #if loss is massive stop\n",
        "        if i > 1 and avg_loss > 4 * lowest_loss:\n",
        "            print(f'from here{i, cost.item}')\n",
        "            return log_lrs, losses, accuracies\n",
        "        if avg_loss < lowest_loss or i == 1:\n",
        "            lowest_loss = avg_loss\n",
        "\n",
        "        accuracies.append(acc_.item())\n",
        "      #accuracies.append(avg_acc)\n",
        "        losses.append(avg_loss)\n",
        "        log_lrs.append(lr)\n",
        "      #step\n",
        "        cost.backward()\n",
        "        optimiser.step()\n",
        "      #update lr\n",
        "        print(f'cost: {cost.item}:.4f, lr: {lr:.4f}, acc: {acc.item():.4f}')\n",
        "        lr *= factor\n",
        "        optimiser.param_groups[0]['lr'] = lr\n",
        "\n",
        "    return log_lrs, losses, accuracies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnp4Y_hqxChc"
      },
      "source": [
        "#Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsb96PnRxEhI"
      },
      "outputs": [],
      "source": [
        "def train(model, optimiser, scheduler=None, epochs=100, store_every=25):\n",
        "    model = model.to(device=device)\n",
        "    for epoch in range(epochs):\n",
        "        train_correct_num = 0\n",
        "        train_total = 0\n",
        "        train_cost_acum = 0.\n",
        "        for mb, (x, y) in enumerate(train_loader, start=1):\n",
        "            model.train()\n",
        "            x = x.to(device=device, dtype=torch.float32)\n",
        "            y = y.to(device=device, dtype=torch.long).squeeze(1)\n",
        "            scores = model(x)\n",
        "            cost = F.cross_entropy(input=scores, target=y)\n",
        "            optimiser.zero_grad()\n",
        "            cost.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "\n",
        "            train_predictions = torch.argmax(scores, dim=1)\n",
        "            train_correct_num += (train_predictions == y).sum()\n",
        "            train_total += cost.item()\n",
        "            if mb%store_every == 0:\n",
        "                val_cost, val_acc, dice, iou = accuracy(model, val_loader)\n",
        "                train_acc = float(train_correct_num) / train_total\n",
        "                train_cost_every = float(train_cost_acum) / mb\n",
        "                print(f'epoch: {epoch}, mb: {mb}, train cost: {train_cost_every:.4f}, val cost: {val_cost:.4f},'\n",
        "                f'train acc: {train_acc:.4f}, val acc: {val_acc:.4f},'\n",
        "                f'dice: {dice}, iou: {iou}')\n",
        "\n",
        "            #save data\n",
        "            #train_acc_history.append(train_acc)\n",
        "            #train_cost_history.append(train_cost_every)\n",
        "        #train_acc = float(train_correct_num) / train_total\n",
        "        #train_cost_every = float(train_cost_acum) / len(train_loader)\n",
        "        #return train_acc_history...etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COcRMTfM12sK"
      },
      "source": [
        "#U-Net model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksNbivdQ14vV"
      },
      "outputs": [],
      "source": [
        "class Conv_3_k(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv1(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZMS72Cc2elq"
      },
      "outputs": [],
      "source": [
        "class Double_Conv(nn.Module):\n",
        "    '''\n",
        "    Double convolution block for de U-Net\n",
        "    '''\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "                          Conv_3_k(channels_in, channels_out),\n",
        "                          nn.BatchNorm2d(channels_out),\n",
        "                          nn.ReLU(),\n",
        "\n",
        "                          Conv_3_k(channels_out, channels_out),\n",
        "                          nn.BatchNorm2d(channels_out),\n",
        "                          nn.ReLU(),\n",
        "                            )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down_Conv(nn.Module):\n",
        "    '''\n",
        "    Down convolution part\n",
        "    '''\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.encoder == nn.Sequential(\n",
        "                        nn.maxPool2d(2,2),\n",
        "                        Double_Conv(channels_in, channels_out)\n",
        "                        )\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Up_Conv(nn.Module):\n",
        "    '''\n",
        "    Up convolutio part\n",
        "    '''\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.upsample_layer = nn.Sequential(\n",
        "                      nn.Upsample(scale_factor=2, mode='bicubic'),\n",
        "                      nn.Conv2d(channels_in, channels_in//2, kernel_size=1, stride=1)\n",
        "                      )\n",
        "\n",
        "        self.decoder = Double_Conv(channels_in, channels_out)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        '''\n",
        "        x1 - upsampled volume\n",
        "        x2 - volume from down sample to concatenate\n",
        "        '''\n",
        "        x1 = self.upsample_layer(x1)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.decoder(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    '''\n",
        "    UNET model\n",
        "    '''\n",
        "    def __init__(self, channels_in, channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.first_conv = Double_Conv(channels_in, channels) #64, 224, 224\n",
        "        self.down_conv1 = Double_Conv(channels, 2*channels) #128, 112, 112\n",
        "        self.down_conv2 = Double_Conv(2*channels, 4*channels) #256, 56, 56\n",
        "        self.down_conv3 = Double_Conv(4*channels, 8*channels) #512, 28, 28\n",
        "\n",
        "        self.middle_conv = Down_Conv(8*channels, 16*channels) #1024, 14, 14\n",
        "\n",
        "        self.up_conv1 = Up_Conv(16*channels, 8*channels)\n",
        "        self.up_conv2 = Up_Conv(8*channels, 4*channels)\n",
        "        self.up_conv3 = Up_Conv(4*channels, 2*channels)\n",
        "        self.up_conv4 = Up_Conv(2*channels, channels)\n",
        "\n",
        "        self.last_conv = nn.Conv2d(channels, num_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.first_conv(x)\n",
        "        x2 = self.down_conv(x1)\n",
        "        x3 = self.down_conv(x2)\n",
        "        x4 = self.down_conv(x3)\n",
        "\n",
        "        x5 = self.middle_conv(x4)\n",
        "\n",
        "        u1 = self.up_conv1(x5, x4)\n",
        "        u2 = self.up_conv2(u1, x3)\n",
        "        u3 = self.up_conv3(u2, x2)\n",
        "        u4 = self.up_conv4(u3, x1)\n",
        "\n",
        "        return self.last_conv(u4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rDlCHUHDzKS"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    x = torch.randn((32, 3, 224, 224))\n",
        "    model = UNET(3, 64, 2)\n",
        "    return model (x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt31i1ahECQa"
      },
      "outputs": [],
      "source": [
        "preds = test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLtvvhvkEFOz"
      },
      "outputs": [],
      "source": [
        "print(preds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j5UEo8tEIYh"
      },
      "source": [
        "#Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPQPrKzLEJ_F"
      },
      "outputs": [],
      "source": [
        "#define the model and look for learning rate\n",
        "torch.manual_seed(42)\n",
        "model = UNET(3, 4, 2)\n",
        "optimiser_unet = torch.optim.SGD(model.parameters(),\n",
        "                                 lr=0.1, momentum=0.95,\n",
        "                                 weight_decay=1e-4)\n",
        "\n",
        "lg_lr, losses, accuracies = find_lr(model, optimiser_unet, start_val=1e-6, end_val=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcfT_o7TE1VH"
      },
      "outputs": [],
      "source": [
        "#plot Loss vs Learning rate\n",
        "f1, ax1 = plt.subplots(figsize=(20,10))\n",
        "ax1.plot(lg_lr, losses)\n",
        "ax1.set_xscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMJ1RDkuFQPj"
      },
      "outputs": [],
      "source": [
        "#define the model and train with scheduler\n",
        "torch.manual_seed(42)\n",
        "model = UNET(3, 4, 2)\n",
        "epochs = 5\n",
        "optimiser_unet = torch.optim.SGD(model.parameters(),\n",
        "                                 lr=0.01, momentum=0.95,\n",
        "                                 weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimiser_unet,\n",
        "                                                max_lr = 1e-1,\n",
        "                                                steps_per_epochs=len(train_loader),\n",
        "                                                epochs=epochs, pct_start=0.43, div_factor=10, final_div_factor=1000,\n",
        "                                                three_phase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQWUypWmGWrF"
      },
      "outputs": [],
      "source": [
        "train(model, optimiser_unet, scheduler, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DRCWsFbGhQ7"
      },
      "source": [
        "#Plot some images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPULNpWXGjHa"
      },
      "outputs": [],
      "source": [
        "imgs_val, masks_val = next(iter(val_loader))\n",
        "imgs_val = imgs_val.to(device, dtype=torch.float32)\n",
        "model = model.to(device)\n",
        "with torch.no_grad():\n",
        "    scores  = model(imgs_val)\n",
        "    preds = torch.argmax(scores, dim=1).float()\n",
        "\n",
        "imgs_val = imgs_val.cpu()\n",
        "preds = preds.cpu()\n",
        "print(preds.shape)\n",
        "plot_mini_batch(imgs_val, preds.unsqueeze(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ2KG97-Hdo8"
      },
      "outputs": [],
      "source": [
        "test_set = Car_Dataset(TEST_PATH, img_transforms=transform_data)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, schuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KiYFlH1H1Xd"
      },
      "outputs": [],
      "source": [
        "imgs_test = next(iter(test_loader))\n",
        "imgs_test = imgs_test.to(device, dtype=torch.float32)\n",
        "model = model.to(device)\n",
        "with torch.no_grad():\n",
        "    scores = model(imgs_test)\n",
        "    preds = torch.argmax(scores, dim=1).float()\n",
        "\n",
        "imgs_test = imgs_test.cpu()\n",
        "preds = preds.cpu()\n",
        "print(preds.shape)\n",
        "plot_mini_batch(imgs_test, preds.unsqueeze(1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}